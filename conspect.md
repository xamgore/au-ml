<style>h3 { border-top: 4px solid #ddd; margin-top: 2rem !important; padding-top: 2rem !important; }</style>
**Baseline** — первый метод, который обычно делается. Зачем его делать? Чтобы потом сравниваться с baseline'ом, а потом уже применять что-то более сложное.

**Ленивое обучение** — мы не придумываем аппроксимирующую функцию, а просто запоминаем датасет и придумываем какое-то правило — отношение к датасету. Например, kNN.

**Ядро** — $K(r)$, не возрастает и положительно на $[0, 1]$.

### Метрические классификаторы. kNN. WkNN. Отбор эталонов. DROP5. Kdtree.

> [Воронцов](http://www.machinelearning.ru/wiki/images/c/c3/Voron-ML-Metric-slides.pdf)

Задача классификации: есть $X$ — множество описаний объектов, $Y$ — множество классов. Существует целевая функция $X \to Y$, значения которой известны на объектах обучающей выборки $D = \{(\vec x_1, y_1) .. (\vec x_n, y_n)\}$. Требуется построить алгоритм, способный классифицировать произвольный объект $\vec x \in X$.

Предположим гипотезу компактности — близкие объекты, как правило, лежат в одном классе. Тогда в метрических пространствах можно построить следующие классификаторы.

#### Метрический алгоритм классификации

Класс нового объекта определяется по суммарному весу каждого класса:

$$h(x, D) = \arg \max\limits_{y\ \in\ Y} \underbrace{\sum\limits_{x_i \in D} [y_i = y]\ w(x_i, x)}_{\Gamma_y(x)} $$

$w(x_i, x)$ — вес (степень важности) соседа $x_i$
$\Gamma_y(x)$ — _близость_ объекта $x$ к классу $y$

#### kNN

* Voting: $w(x_i, x) = 1 \Leftrightarrow x_i$ — один из $k$ ближайших соседей

  * простота реализации (lazy learning)
  * $k$ можно оптимизировать по критерию скользящего контроля (leave-one-out):

  $$ LOO(k, D) = \sum\limits_{x_i \in D} [\ h_k(x_i, D - x_i) \neq y_i\ ]$$
  * неоднозначность классификации при $\Gamma_y(x) = \Gamma_s(x)$ (в сторону наиболее вероятного класса, или наименее болезненный)
  * не учитываются расстояния
  <br>

* Radius Neighbours: $w(x_i, x) = 1 \Leftrightarrow \rho(x_i, x) < R$

#### Weighted kNN

$k$ _взвешенных_ ближайших соседей. Варианты $w$:

* $w(x_i, x) = 1 - \dfrac{\rho(x_i, x)}{const}$
  можно ограничить снизу нулём или рассматривать точки в окружности радиуса $const$.
  <br>

* $w(x_i, x) = q^{-\rho(x_i, x)}$ — экспоненциально убывающие веса
  <br>

* метод парзеновского окна фиксированной ширины: $K\bigg(\dfrac{\rho(x_i, x)}{h}\bigg)$
  <br>

* метод парзеновского окна переменной ширины: $K\bigg(\dfrac{\rho(x_i, x)}{\rho(x^{(k+1)}, x)}\bigg)$


#### Отбор эталонов (Prototype selection)

$$M(x_i) = \Gamma_{y_i}(x_i) - \max\limits_{y\ \in\ Y -\ y_i} \Gamma_y(x_i)$$

Margin (отступ) — оценка того, насколько хорошо точка лежит в своём классе. То есть это разница между близостью правильного класса $y_i$ и наибольшей близостью чужого класса.

![](https://i.imgur.com/vYenfpJ.png)

Шумы — окружены другими классами, эталонные — сидят глубоко в своём классе, или на границе. В качестве эталонов можно брать точки с наибольшим отступом. Тогда классификацию можно делать по эталонам:

$$h(x, \red\Omega) = \arg \max\limits_{y\ \in\ Y} \sum\limits_{x_i \in\ \red\Omega} [y_i = y]\ w(x_i, x) $$

Методы отбора эталонов:

* По навправлению поиска
  * incremental — начинаем с пустого множества эталонов, и добавляем в него
  * decremental — множество эталонов это весь датасет и мы выкидываем из него точки
  * batch (decremental) — когда делаем несколько точек за раз
  * mixed — и выкидываем, и добавляем
  * fixed (mixed) — стараемся сохранить фиксированное количество эталонов
  * <mark>replacement</mark> — выдумываем новую точку

* <mark>По типу выбора</mark>
  * condenstation — выбираем точку, которая хорошо лежит внутри своего класса
  * edition — когда на границе
  * hybrid

#### DROP5

_Decremental Reduction Optimization Procedure_ — метод отбора эталонов. [Псевдокод](https://i.imgur.com/u5RIXWg.png). <mark>Как относится к предыдущей классификации?</mark> (похоже на hybrid)

1. Начинаем с полного набора обучающих точек.
2. Для каждой определим $distance$ — расстояние до ближайшей точки другого класса (близость к врагам).
3. Идем от наименьшего $distance$ к наибольшему.
4. Для каждой точки $P$, смотрим у каких точек $S$ она ближайший сосед
   - $with \mathrel:=$ кол-во правильно классифицированных точек из $S$
   - $without \mathrel:=$ кол-во правильно классифицированных точек из $S$ после удаления $P$
   - если $without \geqslant with$, удаляем $P$

#### Kdtree

Если даже эталонов осталось очень много, можно использовать kdtree — структуру, которая быстро находит ближайших соседей.

* Разбиваем пространство гиперплоскостями, ортогональными одной из координатных осей, последовательно по медианным точкам.

* Поиск соседей начинаем с точек листа, в котором находится точка, если соседей недостаточно – поднимаемся на узел выше.

  <img src="https://i.imgur.com/IcoYW27.png" width="350">


### Кластеризация. kMeans, MeanShift, DBSCAN, Affinity Propagation.

> [Воронцов](http://www.machinelearning.ru/wiki/images/5/52/Voron-ML-Clustering-SSL-slides.pdf)

Кластеризация — разделение множества точек на кластеры по приниципу похожести, кластеризация — обучение без учителя.

Цели кластеризации:
* Упростить дальнейшую обработку данных, разбить на группы схожих объектов и работать с каждой в отдельности
* Сократить объём хранимых данных, оставив по одному представителю от каждого кластера
* Построение иерархии множества объектов (например, таксономия Линнея)
* Выделение нетипичных объектов и аномалий
* Получение новых признаков (если объекты имеют кластерную структуру сами по себе)

Хорошо, когда есть внешняя метрика, например, человек, который может оценить аномалии, или классификатор, который начинает работать лучше, если ему передать признак-кластер. Но если такого нет, то минимизируют расстояния до центроид:

$$ \sum\limits_{x_i}{ \min\limits_{\mu} \big|| x_i - \mu |\big|^2_2 } \to \min $$

#### Алгоритм k-means

1. Инициализируем центры кластеров (случайно или более хитрым образом)
2. Припишем каждую точку к ближайшему центру.
3. Переместим центры кластеров в «центр масс» кластеров:
  $$ \mu_i = \dfrac{1}{|C_i|}\ {\sum\limits_{x\ \in\ C_i}}\ {x} $$
4. Повторяем шаги 2-3 до схождения.

Проблемы:

* неизвестно изначальное кол-во кластеров (а вдруг нужно больше / меньше)
* зависимость от начального положения центроид
* не работает с несферическими (ленточными) кластерами

#### Mean Shift

Вместо количества кластеров задается максимальный размер кластера. Изначально все точки — центры кластеров. Затем итеративно ыперевычисляем центр масс и убираем дубликаты:

$$ m(\mu_i) \mathrel:= \dfrac{\sum\limits_{x\ \in\ N(\mu_i)}{K(x - \mu_i)\ x}}{ \sum\limits_{x\ \in\ N(\mu_i)}{K(x - \mu_i)} } $$

Здесь $N(\mu_i)$ — окрестность центроида $\mu_i$, $K$ — RBF (Radial Basis Function), зачастую Гауссиана: $K(x_j - \mu_i) = e^{-c\ ||x_j - \mu_i||^2_2}$

#### DBSCAN

Количество кластеров не задается. Ищем _объекты плотности_ (core samples) — такие объекты, в $\varepsilon$-окрестности которых есть хотя бы $m$ других объектов. Объединяем объекты плотности на расстоянии $\varepsilon$ и их окружение в кластеры.

Минус: остаётся много некластеризованных объектов. Плюс: кластеры произвольной формы. Быстрый.

#### Affinity Propagation

Не нужно метрическое пространство, нужна лишь матрица попарной близости, для объектов $x_i$ и $x_j$ это $s(i, j)$.

Ответственность $r(i, k)$ — насколько вершина $k$ «морально» готова быть центром кластера вершины $i$ (чем ближе $x_k$ к $x_i$, тем более ответственна).

Доступность $a(i, k)$ — насколько вершина $k$ доступна для того, чтобы быть центром кластера для вершины $i$ (чем больше у $x_k$ рядом точек, тем менее доступна).

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_03.01.png)

После этого самые доступные и ответственные точки выбираем центрами кластеров: $\mu_i = \arg\max\limits_k\ (a(i, k) + r(i, k))$.

Количество кластеров <mark>можно регулировать</mark>, задавая диагональные элементы $s(i, i)$, чем они больше, тем больше будет кластеров.

#### Метрики кластеризации

* внешние:
  * purity — доля точек максимального класса в кластерах
  * метрики классификации
* внутренние:
  * dunn index — минимальное расстояние между центроидами, делённое на максимальное расстояние внутри любой центроиды <mark>→ min</mark>
  * davies-bouldin index → min

### Препроцессинг. Масштабирование. Нормировка. Полиномиальные признаки. One-hot encoding.

<!-- Из-за неравномерных осей kNN в данном случае будет работать плохо.

<img src="https://i.imgur.com/OBVF65y.png" width="350"> -->

#### One-hot encoding

Хорошо описано [здесь](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Коротко: была таблица с категорным признаком, а мы его превратили в фичи. Нужно, чтобы не было проблем, когда модель считает, например, среднее $(1+3)/2 = 2$ и получает, что среднее VW и Honda — Acura.

```
╔════════════╦═════════════════╦════════╗
║ CompanyName Categoricalvalue ║ Price  ║
╠════════════╬═════════════════╣════════║
║ VW         ╬      1          ║ 20000  ║
║ Acura      ╬      2          ║ 10011  ║
║ Honda      ╬      3          ║ 50000  ║
║ Honda      ╬      3          ║ 10000  ║
╚════════════╩═════════════════╩════════╝

╔════╦══════╦══════╦════════╦
║ VW ║ Acura║ Honda║ Price  ║
╠════╬══════╬══════╬════════╬
║ 1  ╬ 0    ╬ 0    ║ 20000  ║
║ 0  ╬ 1    ╬ 0    ║ 10011  ║
║ 0  ╬ 0    ╬ 1    ║ 50000  ║
║ 0  ╬ 0    ╬ 1    ║ 10000  ║
╚════╩══════╩══════╩════════╝
```


#### Пороговые условия. Эффективность по Парето. Presicion-Recall и ROC кривые. AUC.

Логическая закономерность (правило, rule) — функция, определяющая, принадлежит объект классу или нет, действует из пространства входных данных в конечное множество классов.

**Примеры:**
* Пороговые условия: $R(x) = \big[a \leq x \leq b\big]$
* Синдром пороговых условий: $R(x) = \left[\sum\limits_{j\in J} \big[R_j(x_j)\big] \geq d\right]$, где $d = |J|$ — конъюнкция условий ($J$ — набор условий)
* Полуплоскость: $R(x) = \left[\sum\limits_{j\in J} w_jx_j \geq w_0\right]$
* Шар: $R(x) = \big[\rho(x, x_0) \leq w_0\big]$

#### Эффективность по Парето

![](https://i.imgur.com/7kgI3gm.png)

**Оценки правил:**
* Precision: True Positive / (True + False Positive)
* Recall: True Positive / (True Positive + False Negative)
* False positive rate: False Positive / False Positive + True Negative
* Accuracy (точность): (True Positive + Negative) / (All points)
* $F_1$ score — среднее гармоническое precision и recall.

Заметки:
* Если не хотим ложных срабатываний (когда y = 1 — это спам), увеличиваем FP, то есть увеличиваем precision
* TPR → max, FPR → min

**Эффективное по Парето правило** – такое, что нет другого правила, у которого одновременно выше и precision, и recall.

#### Presicion-Recall и ROC кривые

![](https://i.imgur.com/ZF7dBiC.png)

<mark>Почему (0, 0) и (1, 1)</mark>, как интерпретировать precision-recall?

The curve always goes through two points (0,0 and 1,1). 0,0 is where the classifier finds no positives (detects no alarms). In this case it always gets the negative cases right but it gets all positive cases wrong. The second point is 1,1 where everything is classified as positive. So the classifier gets all positive cases right but it gets all negative cases wrong. (I.e. it raises a false alarm on each negative case).

**AUC** — площадь под ROC графиком (чем больше, тем лучше)


### Деревья решений. Информационный выигрыш, критерий Джини. Регуляризация деревьев. Небрежные решающие деревья.

> [Воронцов](http://www.machinelearning.ru/wiki/images/9/97/Voron-ML-Logic-slides.pdf)

#### Дерево решений

<mark>TODO</mark>

Строим дерево, в котором в вершинах находятся различные правила, а в листьях — классы. Запуская исходный объект в дерево по правилам он попадает в один из классов. Для обучения можно пускать тестовую выборку и перекалибровывать правила.

Плюсы: интерпретируемость и простота классификации, тотальность. Минусы: переобучение, высокая чувствительность к шуму.

#### Информационный выигрыш (Information Gain)

$$IG(R) = H(X) - \frac{|R^1|}{|X|}H(R^1) - \frac{|R^0|}{|X|}H(R^0)$$

Вычли из энтропии до разбиения нормированную энтропию левой и правой ветки. При этом, $H(R^1)$ мы хотим минимизировать (когда энтропия равна нулю, у нас всё лежит в одном классе). <mark>R¹ — это что?</mark>

$$H(X) = -\sum\limits_{y\in Y}\dfrac{|x_i:y_i=y|}{|X|}\cdot\log_2\dfrac{|x_i:y_i=y|}{|X|}$$

#### Критерий Джини

$$I_g(X) = \sum\limits_{y\in Y}\dfrac{|x_i:y_i=y|}{|X|}\dfrac{|x_i:y_i\neq y|}{|X|}$$

Вероятность того, что мы ошибёмся, если будем элементам из множества приписывать класс согласно распределению классов в множестве. В ветке из одного элемента, $I_g = 0$.

#### Регуляризация деревьев

На train строим дерево, на test (10-20%) подкручиваем параметры. max depth или минимальный размер листа. <mark>или речь про bagging?</mark>

#### Небрежные решающие деревья

На одном уровне такого дерева правила только по одному и тому же признаку (но с разными условиями).

![](https://i.imgur.com/HEOsJSR.png)

<mark>надо ли про matrix net</mark>?


### Перцептрон. Перцептрон с карманом.

<img src="https://i.imgur.com/30jB6u9.png" width="250">

Пусть точки линейно-разделимы какой-нибудь гиперплоскостью, $w$ — нормаль к ней. Тогда спроецировав точку $x$ на эту нормаль можно по знаку определить класс $y \in \{-1, 1\}$:

$$ h(\mathbf x) = \texttt{sign}\, \bigg( \sum w_i x_i - \text{threshold} \bigg) $$

Или: $\texttt{sign}\,(\mathbf{w}^T \mathbf{x})$. Обучение:
1. Начнём со случайного вектора $w$
2. Находим $\mathbf{x_i}$, что $h(\mathbf{x_i}) \neq y_i$
3. Сдвигаем $\mathbf{w} \leftarrow \mathbf{w}+ y_i\mathbf{x_i}$ (раскроется в $\pm\,\mathbf x^2$, если нужно поменять знак)

#### Перцептрон с карманом

Если точки линейно не разделимы, то обучение перцептрона никогда не остановится. Чтобы это исправить, вводят перцептрон с карманом: при обучении для каждого нового $\mathbf{w}$ мы запоминаем ошибку, и в течение всего алгорима мы храним значение $\mathbf{w}$ с минимальной ошибкой. После этого можно ограничить число итераций и по их окончании взять наилучший вектор $\mathbf{w}$.

### Ошибка внутри и вне выборки. Ошибка обобщения. Неравенство Хёфдинга. Валидация и кросс-валидация.

#### Ошибка внутри и вне выборки

$$E_{in}(h) = \frac 1N \sum\limits_1^N e(h(x_i), f(x_i))$$

матожидание ошибки на всех возможных датасетах:

$$E_{out} = E_x[e(h(x_i), f(x_i))]$$

#### Неравенство Хёфдинга

Неравенство Хёфдинга позволяет оценить ошибку обобщения для одной гипотезы (разделяющей гиперплоскости):

$$P[\underbrace{|E_{in}(h) - E_{out}(h)|}_{\text{ошибка обобщения}} > \varepsilon] \leq 2 e^{-\varepsilon^2N}$$

Для $M$ гипотез:

$$P[|E_{in}(h) - E_{out}(h)| > \varepsilon] \leq \red{M} \cdot 2 e^{-\varepsilon^2N}$$

Но $M$ может быть теоретически равно бесконечности. В случае деревьев кол-во признаков ограничено, способов их разделить ограничено и глубину дерева тоже можно ограничить — ещё ок. С линейными классификаторами две гипотезы могут не сильно отличаться и иметь одинаковую ошибку, поэтому надо рассматривать дихотомии.

<mark>**Неравенство Вапника-Червоненкиса**</mark>

$$P[|E_{in}(h) - E_{out}(h)| > \varepsilon] \leq \red{m_H(2N)}\cdot \red{4}\cdot e^{-\varepsilon^{\red{\frac 18}}N}$$



<mark>что там с графиком на 79?</mark> При усложнении модели (увеличении гипотез) $E_{in}$ может сильно уменьшаться, в то время как $E_{out}$ начнет увеличиваться (переобучение).

#### Валидация и кросс-валидация

Чтобы убдиться, что все $M$ гипотез работают хорошо, нужно, чтобы размер датасета $N \geqslant 10\ d_{VC}$. Но оценить размерность не всегда просто, поэтому используют валидацию.

После обучения выбирает одну лучшую гипотезу, и её тестируем на val датасете. Поскольку тестируем одну гипотезу, это неравенство работает без какого-либо коэффициента. Размер val обычно 20%:

$$P[{|E_{val}(h) - E_{out}(h)|}_{} > \varepsilon] \leq 2 e^{-\varepsilon^2N}$$

**Кросс-валидация:** делим датасет на 10 равных датасетов. Далее каждый из этих десяти по очереди назначаем валидационным, а остальные — тренировочными. В результате выбираем лучший классификатор.

### Гипотезы и дихотомии. Функция роста. Точка поломки. Доказательство полиномиальности функции роста в присутствии точки поломки.

Пусть есть бесконечное множество гипотез $H$, где каждая $h: X\rightarrow \{-1,+1\}$. Если применить $h \in H$ к кортежу $\mathbf{x_1}, \mathbf{x_2}, .., \mathbf{x_N}$, мы получим кортеж $h(\mathbf{x_1}), h(\mathbf{x_2}), .., h(\mathbf{x_N})$ из $\pm1$, который называется _дихотомией_. Две разные гипотезы могут сгенерировать одинаковую дихотомию.

$$ H(\mathbf{x_1}, \mathbf{x_2}, .., \mathbf{x_N}) = \{\ (h(\mathbf{x_1}), h(\mathbf{x_2}), .., h(\mathbf{x_N})) \mid h \in H \} $$

$$m_H(N) = \max\limits_{x_1,\cdots,x_N}|H(\mathbf{x_1},\cdots,\mathbf{x_N})|$$

Так как $H(\mathbf{x_1},\cdots,\mathbf{x_N}) \subset \{ -1, 1\}^N$ (множество всех возможных дихотомий на $N$ точках), то $m_H(N) \leq |\{ -1, 1\}^N| = 2^N$.

Если не существует датасета размера $k$, который мог бы быть разбит гипотезами $H$ на $2^k$ дихотомий, то говорят, что $k$ — _точка поломки_ для $H$.


#### Доказательство полиномиальности функции роста в присутствии точки поломки

$B(N, 1) = 1$, так как мы одну точку можем единственным образом классифицировать, значит, и все $N$ точек тоже.

$B(1, k) = 2\quad \text{for}\ k > 1$, одну точку можем классифицировать по-разному, двумя способами.

Предположим теперь, что $N \geqslant 2$ и $k \geqslant 2$, будем искать рекурсию. Представим $B(N, k)$ дихотомий в таблице. $S_2^+$ и $S_2^-$ — первые $N-1$ точкек в дихотомиях совпадает, $S_1$ — нет, уникальные.

<center><img src="https://i.imgur.com/joKDxD3.png" width="400"></center>

Ясно, что $B(N, k) = \alpha + 2\beta$

* Заметим, что $S_1 \cup S_2^+$ образуют уникальные дихотомии для первых $N-1$ точек. И в них $k$ — всё так же точка поломки; если бы это было не так, то мы бы и в $N$ точках могли бы найти $2^k$ дихотомий, но это не так.
  $\Rightarrow \alpha + \beta \leqslant B(N - 1, k)$
  <br>

* Нет подмножества размера $k-1$ из первых $N-1$ точек, в котором были бы все $2^{k-1}$ дихотомии из $S_2^+$. Если бы оно было, то взяв соответствующие дихотомии из $S_2^-$, и добавив точку $\mathbf x_N$ мы получили бы $2^k$ дихотомий для $N$ точек, но этого не может быть, так как $k$ — точка поломки по определению $B(N, k)$.
  $\Rightarrow \beta \leqslant B(N - 1, k - 1)$
  <br>

Из этих двух:

$$ B(N, k) \leqslant B(N - 1, k) + B(N - 1, k - 1) \leqslant \sum\limits_{i = 0}^{k-1} \binom{N}{i} $$

Последнее доказывается для случая $k \geqslant 2, N \geqslant 2$ по индукции. База выполняется.

<center><img src="https://i.imgur.com/3qvqfoF.png" style="max-width:600px"></center>

<mark>Сумма — полином</mark>, поэтому и верхняя граница на функцию роста — полином.

<center><img src="https://i.imgur.com/3XEwflU.png" style="max-width:600px"></center>

### Размерность Вапника-Червоненкиса. Размерность Вапника-Червоненкиса для перцептрона.

Размерность Вапника-Червоненкиса $d_{VC}(H)$ для набора гипотез $H$ -- это максимальное число точек, для которого можно получить все возможные дихотомии (функция роста $m_H(N) = 2^N$).

$d_{VC}(H) = k - 1$, где $k$ -- точка поломки

Справедливо следующее соотношение между VC-размерностью и функцией роста:

$m_H(N)\leq \sum\limits_{i=0}^{d_{VC}}C_N^i\leq N^{d_{VC}} + 1$

##### Размерность Вапника-Червоненкиса для перцептрона

Для перцептрона размерности $d$, $d_{VC} = d + 1$.

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_05.03.png)

<mark>снизу показали $d_{vc} \leqslant d + 1$, как сверху показали, что $d_{vc} \geqslant d + 1$?

![](https://i.imgur.com/TyuqOGW.png)


### Логистическая регрессия. Градиентный спуск.

> 98 стр. | [Воронцов](http://www.machinelearning.ru/wiki/images/5/53/Voron-ML-Lin-SG.pdf)

<mark>ЧТО ЭТО ВООБЩЕ</mark>

Логистическая функция (сигмоида):

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_06.01.png)

**Логистическая регрессия** — задача классификации в виде простой нейронной сети из одного слоя с одним нейроном

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_06.02.png)

$w$ -- веса входного сигнала. Ставим цель -- максимизировать правдоподобие. Или минимизировать Loss:

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_06.03.png)

### Градиентный спуск

Тут $C(w)$ -- Это функция потерь (то же самое, что и $L(w)$ выше).

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_06.04.png)

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_06.05.png)

Стохастический градиентный спуск -- считаем градиент не по всем точкам, а только по некоторым (батчам). Заметно ускоряет работу, при этом работает не сильно хуже, чем если бы считали по всем. Из-за того, что может проскачить шумы, может работать лучше. Обычный градиентный спуск будет учитывать все точки $x$, и шумы тоже.
