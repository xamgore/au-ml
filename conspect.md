<style>h3 { border-top: 4px solid #ddd; margin-top: 2rem !important; padding-top: 2rem !important; }</style>
**Baseline** — первый метод, который обычно делается. Зачем его делать? Чтобы потом сравниваться с baseline'ом, а потом уже применять что-то более сложное.

**Ленивое обучение** — мы не придумываем аппроксимирующую функцию, а просто запоминаем датасет и придумываем какое-то правило — отношение к датасету. Например, kNN.

**Ядро** — $K(r)$, не возрастает и положительно на $[0, 1]$.

### Метрические классификаторы. kNN. WkNN. Отбор эталонов. DROP5. Kdtree.

> [Воронцов](http://www.machinelearning.ru/wiki/images/c/c3/Voron-ML-Metric-slides.pdf)

Задача классификации: есть $X$ — множество описаний объектов, $Y$ — множество классов. Существует целевая функция $X \to Y$, значения которой известны на объектах обучающей выборки $D = \{(\vec x_1, y_1) .. (\vec x_n, y_n)\}$. Требуется построить алгоритм, способный классифицировать произвольный объект $\vec x \in X$.

Предположим гипотезу компактности — близкие объекты, как правило, лежат в одном классе. Тогда в метрических пространствах можно построить следующие классификаторы.

#### Метрический алгоритм классификации

Класс нового объекта определяется по суммарному весу каждого класса:

$$h(x, D) = \arg \max\limits_{y\ \in\ Y} \underbrace{\sum\limits_{x_i \in D} [y_i = y]\ w(x_i, x)}_{\Gamma_y(x)} $$

$w(x_i, x)$ — вес (степень важности) соседа $x_i$
$\Gamma_y(x)$ — _близость_ объекта $x$ к классу $y$

#### kNN

* Voting: $w(x_i, x) = 1 \Leftrightarrow x_i$ — один из $k$ ближайших соседей

  * простота реализации (lazy learning)
  * $k$ можно оптимизировать по критерию скользящего контроля (leave-one-out):

  $$ LOO(k, D) = \sum\limits_{x_i \in D} [\ h_k(x_i, D - x_i) \neq y_i\ ]$$
  * неоднозначность классификации при $\Gamma_y(x) = \Gamma_s(x)$ (в сторону наиболее вероятного класса, или наименее болезненный)
  * не учитываются расстояния
  <br>

* Radius Neighbours: $w(x_i, x) = 1 \Leftrightarrow \rho(x_i, x) < R$

#### Weighted kNN

$k$ _взвешенных_ ближайших соседей. Варианты $w$:

* $w(x_i, x) = 1 - \dfrac{\rho(x_i, x)}{const}$
  можно ограничить снизу нулём или рассматривать точки в окружности радиуса $const$.
  <br>

* $w(x_i, x) = q^{-\rho(x_i, x)}$ — экспоненциально убывающие веса
  <br>

* метод парзеновского окна фиксированной ширины: $K\bigg(\dfrac{\rho(x_i, x)}{h}\bigg)$
  <br>

* метод парзеновского окна переменной ширины: $K\bigg(\dfrac{\rho(x_i, x)}{\rho(x^{(k+1)}, x)}\bigg)$


#### Отбор эталонов (Prototype selection)

$$M(x_i) = \Gamma_{y_i}(x_i) - \max\limits_{y\ \in\ Y -\ y_i} \Gamma_y(x_i)$$

Margin (отступ) — оценка того, насколько хорошо точка лежит в своём классе. То есть это разница между близостью правильного класса $y_i$ и наибольшей близостью чужого класса.

![](https://i.imgur.com/vYenfpJ.png)

Шумы — окружены другими классами, эталонные — сидят глубоко в своём классе, или на границе. В качестве эталонов можно брать точки с наибольшим отступом. Тогда классификацию можно делать по эталонам:

$$h(x, \red\Omega) = \arg \max\limits_{y\ \in\ Y} \sum\limits_{x_i \in\ \red\Omega} [y_i = y]\ w(x_i, x) $$

Методы отбора эталонов:

* По навправлению поиска
  * incremental — начинаем с пустого множества эталонов, и добавляем в него
  * decremental — множество эталонов это весь датасет и мы выкидываем из него точки
  * batch (decremental) — когда делаем несколько точек за раз
  * mixed — и выкидываем, и добавляем
  * fixed (mixed) — стараемся сохранить фиксированное количество эталонов
  * <mark>replacement</mark> — выдумываем новую точку

* <mark>По типу выбора</mark>
  * condenstation — выбираем точку, которая хорошо лежит внутри своего класса
  * edition — когда на границе
  * hybrid

#### DROP5

_Decremental Reduction Optimization Procedure_ — метод отбора эталонов. [Псевдокод](https://i.imgur.com/u5RIXWg.png). <mark>Как относится к предыдущей классификации?</mark> (похоже на hybrid)

1. Начинаем с полного набора обучающих точек.
2. Для каждой определим $distance$ — расстояние до ближайшей точки другого класса (близость к врагам).
3. Идем от наименьшего $distance$ к наибольшему.
4. Для каждой точки $P$, смотрим у каких точек $S$ она ближайший сосед
   - $with \mathrel:=$ кол-во правильно классифицированных точек из $S$
   - $without \mathrel:=$ кол-во правильно классифицированных точек из $S$ после удаления $P$
   - если $without \geqslant with$, удаляем $P$

#### Kdtree

Если даже эталонов осталось очень много, можно использовать kdtree — структуру, которая быстро находит ближайших соседей.

* Разбиваем пространство гиперплоскостями, ортогональными одной из координатных осей, последовательно по медианным точкам.

* Поиск соседей начинаем с точек листа, в котором находится точка, если соседей недостаточно – поднимаемся на узел выше.

  <img src="https://i.imgur.com/IcoYW27.png" width="350">


### Кластеризация. kMeans, MeanShift, DBSCAN, Affinity Propagation.

> [Воронцов](http://www.machinelearning.ru/wiki/images/5/52/Voron-ML-Clustering-SSL-slides.pdf)

Кластеризация — разделение множества точек на кластеры по приниципу похожести, кластеризация — обучение без учителя.

Цели кластеризации:
* Упростить дальнейшую обработку данных, разбить на группы схожих объектов и работать с каждой в отдельности
* Сократить объём хранимых данных, оставив по одному представителю от каждого кластера
* Построение иерархии множества объектов (например, таксономия Линнея)
* Выделение нетипичных объектов и аномалий
* Получение новых признаков (если объекты имеют кластерную структуру сами по себе)

Хорошо, когда есть внешняя метрика, например, человек, который может оценить аномалии, или классификатор, который начинает работать лучше, если ему передать признак-кластер. Но если такого нет, то минимизируют расстояния до центроид:

$$ \sum\limits_{x_i}{ \min\limits_{\mu} \big|| x_i - \mu |\big|^2_2 } \to \min $$

#### Алгоритм k-means

1. Инициализируем центры кластеров (случайно или более хитрым образом)
2. Припишем каждую точку к ближайшему центру.
3. Переместим центры кластеров в «центр масс» кластеров:
  $$ \mu_i = \dfrac{1}{|C_i|}\ {\sum\limits_{x\ \in\ C_i}}\ {x} $$
4. Повторяем шаги 2-3 до схождения.

Проблемы:

* неизвестно изначальное кол-во кластеров (а вдруг нужно больше / меньше)
* зависимость от начального положения центроид
* не работает с несферическими (ленточными) кластерами

#### Mean Shift

Вместо количества кластеров задается максимальный размер кластера. Изначально все точки — центры кластеров. Затем итеративно ыперевычисляем центр масс и убираем дубликаты:

$$ m(\mu_i) \mathrel:= \dfrac{\sum\limits_{x\ \in\ N(\mu_i)}{K(x - \mu_i)\ x}}{ \sum\limits_{x\ \in\ N(\mu_i)}{K(x - \mu_i)} } $$

Здесь $N(\mu_i)$ — окрестность центроида $\mu_i$, $K$ — RBF (Radial Basis Function), зачастую Гауссиана: $K(x_j - \mu_i) = e^{-c\ ||x_j - \mu_i||^2_2}$

#### DBSCAN

Количество кластеров не задается. Ищем _объекты плотности_ (core samples) — такие объекты, в $\varepsilon$-окрестности которых есть хотя бы $m$ других объектов. Объединяем объекты плотности на расстоянии $\varepsilon$ и их окружение в кластеры.

Минус: остаётся много некластеризованных объектов. Плюс: кластеры произвольной формы. Быстрый.

#### Affinity Propagation

Не нужно метрическое пространство, нужна лишь матрица попарной близости, для объектов $x_i$ и $x_j$ это $s(i, j)$.

Ответственность $r(i, k)$ — насколько вершина $k$ «морально» готова быть центром кластера вершины $i$ (чем ближе $x_k$ к $x_i$, тем более ответственна).

Доступность $a(i, k)$ — насколько вершина $k$ доступна для того, чтобы быть центром кластера для вершины $i$ (чем больше у $x_k$ рядом точек, тем менее доступна).

![](https://demiurg906.github.io/au_docs_ijJ9eOA8/Machine_learning/images/lec_03.01.png)

После этого самые доступные и ответственные точки выбираем центрами кластеров: $\mu_i = \arg\max\limits_k\ (a(i, k) + r(i, k))$.

Количество кластеров <mark>можно регулировать</mark>, задавая диагональные элементы $s(i, i)$, чем они больше, тем больше будет кластеров.

#### Метрики кластеризации

* внешние:
  * purity — доля точек максимального класса в кластерах
  * метрики классификации
* внутренние:
  * dunn index — минимальное расстояние между центроидами, делённое на максимальное расстояние внутри любой центроиды <mark>→ min</mark>
  * davies-bouldin index → min

### Препроцессинг. Масштабирование. Нормировка. Полиномиальные признаки. One-hot encoding.

<!-- Из-за неравномерных осей kNN в данном случае будет работать плохо.

<img src="https://i.imgur.com/OBVF65y.png" width="350"> -->

#### One-hot encoding

Хорошо описано [здесь](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Коротко: была таблица с категорным признаком, а мы его превратили в фичи. Нужно, чтобы не было проблем, когда модель считает, например, среднее $(1+3)/2 = 2$ и получает, что среднее VW и Honda — Acura.

```
╔════════════╦═════════════════╦════════╗
║ CompanyName Categoricalvalue ║ Price  ║
╠════════════╬═════════════════╣════════║
║ VW         ╬      1          ║ 20000  ║
║ Acura      ╬      2          ║ 10011  ║
║ Honda      ╬      3          ║ 50000  ║
║ Honda      ╬      3          ║ 10000  ║
╚════════════╩═════════════════╩════════╝

╔════╦══════╦══════╦════════╦
║ VW ║ Acura║ Honda║ Price  ║
╠════╬══════╬══════╬════════╬
║ 1  ╬ 0    ╬ 0    ║ 20000  ║
║ 0  ╬ 1    ╬ 0    ║ 10011  ║
║ 0  ╬ 0    ╬ 1    ║ 50000  ║
║ 0  ╬ 0    ╬ 1    ║ 10000  ║
╚════╩══════╩══════╩════════╝
```


#### Пороговые условия. Эффективность по Парето. Presicion-Recall и ROC кривые. AUC.

Логическая закономерность (правило, rule) — функция, определяющая, принадлежит объект классу или нет, действует из пространства входных данных в конечное множество классов.

**Примеры:**
* Пороговые условия: $R(x) = \big[a \leq x \leq b\big]$
* Синдром пороговых условий: $R(x) = \left[\sum\limits_{j\in J} \big[R_j(x_j)\big] \geq d\right]$, где $d = |J|$ — конъюнкция условий ($J$ — набор условий)
* Полуплоскость: $R(x) = \left[\sum\limits_{j\in J} w_jx_j \geq w_0\right]$
* Шар: $R(x) = \big[\rho(x, x_0) \leq w_0\big]$

#### Эффективность по Парето

![](https://i.imgur.com/7kgI3gm.png)

**Оценки правил:**
* Precision: True Positive / (True + False Positive)
* Recall: True Positive / (True Positive + False Negative)
* False positive rate: False Positive / False Positive + True Negative
* Accuracy (точность): (True Positive + Negative) / (All points)
* $F_1$ score — среднее гармоническое precision и recall.

Заметки:
* Если не хотим ложных срабатываний (когда y = 1 — это спам), увеличиваем FP, то есть увеличиваем precision
* TPR → max, FPR → min

**Эффективное по Парето правило** – такое, что нет другого правила, у которого одновременно выше и precision, и recall.

#### Presicion-Recall и ROC кривые

![](https://i.imgur.com/ZF7dBiC.png)

<mark>Почему (0, 0) и (1, 1)</mark>, как интерпретировать precision-recall?

**AUC** — площадь под ROC графиком (чем больше, тем лучше)


### Деревья решений. Информационный выигрыш, критерий Джини. Регуляризация деревьев. Небрежные решающие деревья.

> [Воронцов](http://www.machinelearning.ru/wiki/images/9/97/Voron-ML-Logic-slides.pdf)

#### Дерево решений

<mark>TODO</mark>

Строим дерево, в котором в вершинах находятся различные правила, а в листьях — классы. Запуская исходный объект в дерево по правилам он попадает в один из классов. Для обучения можно пускать тестовую выборку и перекалибровывать правила.

Плюсы: интерпретируемость и простота классификации, тотальность. Минусы: переобучение, высокая чувствительность к шуму.

#### Информационный выигрыш (Information Gain)

$$IG(R) = H(X) - \frac{|R^1|}{|X|}H(R^1) - \frac{|R^0|}{|X|}H(R^0)$$

Вычли из энтропии до разбиения нормированную энтропию левой и правой ветки. При этом, $H(R^1)$ мы хотим минимизировать (когда энтропия равна нулю, у нас всё лежит в одном классе). <mark>R¹ — это что?</mark>

$$H(X) = -\sum\limits_{y\in Y}\dfrac{|x_i:y_i=y|}{|X|}\cdot\log_2\dfrac{|x_i:y_i=y|}{|X|}$$

#### Критерий Джини

$$I_g(X) = \sum\limits_{y\in Y}\dfrac{|x_i:y_i=y|}{|X|}\dfrac{|x_i:y_i\neq y|}{|X|}$$

Вероятность того, что мы ошибёмся, если будем элементам из множества приписывать класс согласно распределению классов в множестве. В ветке из одного элемента, $I_g = 0$.

#### Регуляризация деревьев

На train строим дерево, на test (10-20%) подкручиваем параметры. max depth или минимальный размер листа. <mark>или речь про bagging?</mark>

#### Небрежные решающие деревья

На одном уровне такого дерева правила только по одному и тому же признаку (но с разными условиями).

![](https://i.imgur.com/HEOsJSR.png)

<mark>надо ли про matrix net</mark>?


### Перцептрон. Перцептрон с карманом.
